---
permalink: process.html
title: Integration Points
layout: post
desc: Systems that Integrate with Collins
---

Several systems integrate with collins which include:

 * Puppet
 * Deploy Tool
 * Jet Pants
 * Config Generators
 * Reconciler
 * Decommissioner
 * Provisioner

As mentioned elsewhere, collins depends on a set of decoupled agents to activate state changes.
This document details how (or if) each agent interacts with collins and each other, as well as the
general asset lifecycle.

## tl;dr

Before a web can move from allocated to provisioned it must pass an automated battery of tests which
ensure it is in a reasonable state.

Once a machine is moved into an allocated state, it won't start taking production traffic until it is
deployed to for the first time because the proxy configurations are generated based on that information.

Once a machine gets deployed to during a normal deploy, the next time the proxy configs are generated
it will start getting traffic.

## Process

Users generally only specify 3 states for assets. Provisioning, when a user starts the provisioning process.
Cancelled, when a user cancels an asset. And maintenance, when a user puts an asset into maintenance mode. All
remaining states are managed through these decoupled external agents. This process is detailed below.

<a href="images/collins_process.png"><img src="images/collins_process_small.png"></a>

Other external agents may depend on various states of assets to know how to behave, but do not in general modify
the asset status.

## Users

Users can impact the status of an asset in 3 ways:

 * Request an Incomplete (spare pool) host, which moves an asset from incomplete to new. This request is known as activation.
 * Provision an unallocated host, moving an asset from unallocated to provisioning
 * Cancelling a host, moving it into a cancelled state

Other status management is handled elsewhere.

## Reconciler

The reconciler is responsible for several tasks which include:

 * Adding new assets to collins (asset becomes incomplete) found in any external provider databases
 * Moving hosts from new to provisioning once they are available to us for use
 * Moving hosts from provisioned to allocated once a host is 'sane'
 * Ensure consistent values between external vendor databases and the internal collins database (credentials, hostnames, etc)
 * Finding cancelled hosts in collins that are no longer in the external provider database, and moving it to decommissioned

### Adding new assets

Assets can be added to collins by the reconciler either as incomplete or unallocated. Hosts that are immediately available
for use are created as unallocated, while hosts that will need to be 'requested' are given a status of incomplete. If the
host is created as unallocated the hardware is inventoried as it is created.

### New to Provisioning

Once the activated host is available, the reconciler will see this and begin the provisioning process. Once
the provisiong begins the asset status will change the provisioning until the provision is done at which point the
status will be changed to provisioned.

### Provisioned to Allocated

The reconciler has a built in test suite that is specific to each node class (this is the type of server setup requested during the provisioning dialog).
The reconciler runs the appropriate suite against each provisioned host to determine whether it is safe to mark the host as allocated or not.
By default, every node class has the following tests run against it:

 * SSH remote access is available
 * The asset has a hostname in collins
 * The asset has a DNS entry that matches the specified hostname
 * The asset is monitored (and green) in nagios
 * The asset is in trending (opentsdb)

Some classes of machine have specific tests. For example, on database nodes we ensure that the disk scheduler is set to deadline.

No host can be promoted from provisioned to allocated until all tests pass. Any failed tests are logged against the asset.

### Data Consistency

The reconciler ensures that the following information is consistent between collins and external data sources:

 * Credentials (username and password)
 * Hardware inventory (if unavailable for an asset, since this is immutable once stored)
 * Rack, room, rack position
 * IP address information
 * System cost
 * IPMI information

In the case of a conflict (data is different, not just empty in collins) we generally prefer the data found in collins and assume it's
more accurate. In certain cases, such as the rack or rack position, we prefer the providers data.

### Dead Hosts

Once a host has been reclaimed by the service provider, the reconciler will delete the host via the API. A collins delete only marks the
asset as decommissioned and optionally purges some (or all) asset tags and IPMI information. What gets deleted is configurable. Only hosts
that are decommissioned or cancelled are subject to this process. If a host is in a different state, you will get a loud warning via hipchat
and no action will be taken on the host.

## Provisioner

The provisioner manages asset provisioning. It leverages cobbler to provide kickstart profiles, and collins to provide the data needed
to generate those kickstart profiles. Cobbler will likely be deprecated in the near future. The provisioner process is roughly:

 * Verify asset sanity (can we SSH into it?)
 * Delete the asset in cobbler and re-create it as specified by the provisioning request
 * Ensure hostname consistency in collins and external provider databases
 * Set the status to Provisioning (assuming the asset isn't being reprovisioned for destruction)
 * Set the nodeclass in collins, if required, as specified by the provisioning request
 * Reboot the machine onto the appropriate kickstart image
 * Clean up func and puppet, removing any old certificates or yaml files
 * Reprovision or reset any network related ports

PXE boots are not currently supported, so remote SSH access is required.

This process is leveraged by both the reconciler as well as decommissioner as well.

## Decommissioner

The decommissioner is used for processing cancelled hosts as part of the out take process. This process
probably deserves its own document, as it's somewhat complex compared to the other processes. There are
a few key take aways for how this process behaves:

 * Without rewriting the agent, no assets can be processed that aren't cancelled
 * Machines are powered off for a minimum of one day before they are rebooted for cleaning
 * The cleaning process is completely destructive and unrecoverable

The reason this process is fairly complex is to ensure that assets are decommissioned sanely and safely.
To accomplish this, there are 8 cleaning related states that a host can be in that are managed over the
course of a week. The cleaning state is kept in an asset tag called `POWER_STATUS` which indicates the
phase an asset is in. Note that the decommissioner implementation prevents for instance the case where
we try and power on a machine for cleaning and the power on command fails yet we mark the host as cleaning.
Only successful operations can advance the power status of an asset.

Checking in on a normal server every day, the happy path flow would reflect the following power status on each day.

 * on - We see the host is on, and power it down, but record the previous state
 * cleaning - We see the host is off, and power it on for cleaning
 * cleaning2 - We start the cleaning process, kicking the asset onto a destructive image
 * cleaned - The cleaning phase is completed and the asset is now cleaned

Once a cancelled asset is cleaned, it is ignored until the reconciler sees it should be decommissioned.

If remote IPMI power functions are for some reason unavailable the sad path is followed. The sad
path is followed where the power status transitions as follows:

 * unknown - Unable to gather the current power state of the asset
 * unmanaged1 - Still unable, let's try again tomorrow
 * unmanaged2 - Wow, still can't check the power state.
 * unmanaged - Forget it, we're never going to get the power state.

Once a cancelled asset is unmanaged, the decommissioner will ignore it indefinitely. These hosts have
to be cleaned by hand. The reason we have several retries is that depending on the collins power status
plugin being used, the system providing the status may be down for some period of time.

Of course, you can force the decommissioner to misbehave and rush through the entire process but in
general the defaults should be fine for most people.

This process should have been named cleaner, not decommissioner, but c'est la vie.

## Puppet

Puppet moves an asset from `Provisioning` to `Provisioned` once it has
successfully run through the provisioning process. It also has facts available
to it such as the collins assigned primary role and pool. These facts can be
used in puppet manifests.

## Config Operators (JetPants, Deploys, Configs)

We leverage the data in collins in the following ways:

 * JetPants maintains appropriate database roles for assets during promotions and such (slave, master, etc)
 * The deploy tools look at allocated hosts (with appropriate pools or secondary roles) to determine what machines to deploy to
 * DNS, upstream proxies (ha proxy, varnish) and nagios configurations

### JetPants

JetPants is the tool we use internally for database management. Currently the integration is used for
knowing databases available for pulling into a pool in the case where a machine in a database pool fails.

It also maintains the role of each database server (backup, slave, master, etc).

### Deploy Tools

The deploy tool pulls assets from collins that are allocated and match an appropriate primary role and
pool, as well as an optional secondary role. We use secondary roles for grouping hosts together (e.g.
web a has 5 hosts, web b has 50 hosts, etc).

The deploy tool updates each asset with the sha1 of the version of software that was deployed to it. If a deploy
is unsuccessful for some reason, the sha will not be updated.

For the tumblr web application, we update a configuration asset named tumblr with the most recently
deployed sha for the applilcation, configuration, and credentials. This sha information is used by
the proxy configuration generation. 

For services deployed with the deploy tool, the sha on the asset may or may not still be updated. The same
logic for what hosts should get deployed to is identical to that of the web app.

### Configs

The DNS generation pulls all hosts that are not decommissioned, new or incomplete into the appropriate DNS
zone files. The entry in the zone file is based on the hostname in collins, and the private IP address in
collins. This generation is performed only when a change has occurred and is run via cron currently.

The nagios config generation is still in development but includes all hosts in allocated or provisioned states.
This eventually will be both partially triggerred (delete hosts that are being provisioned) and partially
via check (don't alert on hosts in maintenance).

The proxy config generation produces different results depending on whether the proxy is HAProxy or varnish,
but the logic is the same. We'll use the dashboard setup as an example since it includes both HAProxy and varnish.
The process is roughly as follows:

 1. Fetch current sha of production software from collins
 2. Fetch all hosts that are: Allocated, have a primary role of `TUMBLR_APP`, have a pool of `DASHBOARD_POOL`, and have the most recent version of the production software
 3. Do the above step for varnish as well as for HAProxy 

The above generation mechanism, combined with the deploy tool, means that we should never have any production hosts with bad (or old) software taking traffic. This is useful in the case where you take a host out for maintenance and then put it back in the web pool. That host should not start taking production traffic until it has the most recent version of software.


